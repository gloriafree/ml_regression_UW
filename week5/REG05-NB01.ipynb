{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 2","language":"python","name":"python2"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.15"},"colab":{"name":"REG05-NB01.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"0taPfxTXVkUX"},"source":["# Regression Week 5: Feature Selection and LASSO (Interpretation)"]},{"cell_type":"markdown","metadata":{"id":"JWnuYfxBVkUX"},"source":["In this notebook, you will use LASSO to select features, building on a pre-implemented solver for LASSO (using Turi Create, though you can use other solvers). You will:\n","* Run LASSO with different L1 penalties.\n","* Choose best L1 penalty using a validation set.\n","* Choose best L1 penalty using a validation set, with additional constraint on the size of subset.\n","\n","In the second notebook, you will implement your own LASSO solver, using coordinate descent. "]},{"cell_type":"markdown","metadata":{"id":"ZVVNM6YkVkUX"},"source":["# Fire up Turi Create"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"yD7y-RAjVkUX","executionInfo":{"status":"ok","timestamp":1606541640754,"user_tz":300,"elapsed":61403,"user":{"displayName":"Gloria Lee","photoUrl":"","userId":"14673960919881103227"}},"outputId":"199c1803-e1d2-4cc8-b9f8-915dd019dfde"},"source":["!pip install turicreate\n","import turicreate"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting turicreate\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/38/77a081ce35f012bd4789551db9e5196e766a2641cb05114ded9cc268182c/turicreate-6.4.1-cp27-cp27mu-manylinux1_x86_64.whl (91.9MB)\n","\u001b[K     |████████████████████████████████| 91.9MB 41kB/s \n","\u001b[?25hCollecting pillow>=5.2.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/12/ad/61f8dfba88c4e56196bf6d056cdbba64dc9c5dfdfbc97d02e6472feed913/Pillow-6.2.2-cp27-cp27mu-manylinux1_x86_64.whl (2.1MB)\n","\u001b[K     |████████████████████████████████| 2.1MB 49.1MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python2.7/dist-packages (from turicreate) (1.16.4)\n","Requirement already satisfied: requests>=2.9.1 in /usr/local/lib/python2.7/dist-packages (from turicreate) (2.23.0)\n","Collecting tensorflow<2.1.0,>=2.0.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/17/08/1ff15637a03b1565dd6cb0916b3ca6873db3a1fc69be0ed851be936e5633/tensorflow-2.0.0-cp27-cp27mu-manylinux2010_x86_64.whl (86.3MB)\n","\u001b[K     |████████████████████████████████| 86.3MB 61kB/s \n","\u001b[?25hRequirement already satisfied: prettytable==0.7.2 in /usr/local/lib/python2.7/dist-packages (from turicreate) (0.7.2)\n","\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', error(104, 'Connection reset by peer'))': /simple/llvmlite/\u001b[0m\n","Collecting llvmlite==0.31.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/07/90cd9cdd43b287960b16dbf7929bff2267dc0b6647e8e0117a4937b19620/llvmlite-0.31.0-cp27-cp27mu-manylinux1_x86_64.whl (20.2MB)\n","\u001b[K     |████████████████████████████████| 20.2MB 1.3MB/s \n","\u001b[?25hRequirement already satisfied: pandas>=0.23.2 in /usr/local/lib/python2.7/dist-packages (from turicreate) (0.24.2)\n","Requirement already satisfied: decorator>=4.0.9 in /usr/local/lib/python2.7/dist-packages (from turicreate) (4.4.0)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python2.7/dist-packages (from turicreate) (1.2.2)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python2.7/dist-packages (from turicreate) (1.15.0)\n","Requirement already satisfied: resampy==0.2.1 in /usr/local/lib/python2.7/dist-packages (from turicreate) (0.2.1)\n","Requirement already satisfied: numba<0.51.0 in /usr/local/lib/python2.7/dist-packages (from turicreate) (0.40.1)\n","Collecting coremltools==3.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/9b/f6feac5d9369c68ee718d3c3140435452773a36ff08ef3bcc4fe378e8676/coremltools-3.3-cp27-none-manylinux1_x86_64.whl (3.4MB)\n","\u001b[K     |████████████████████████████████| 3.4MB 46.7MB/s \n","\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python2.7/dist-packages (from requests>=2.9.1->turicreate) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python2.7/dist-packages (from requests>=2.9.1->turicreate) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python2.7/dist-packages (from requests>=2.9.1->turicreate) (2019.6.16)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python2.7/dist-packages (from requests>=2.9.1->turicreate) (2.8)\n","Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python2.7/dist-packages (from tensorflow<2.1.0,>=2.0.0->turicreate) (0.2.2)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow<2.1.0,>=2.0.0->turicreate) (1.15.0)\n","Collecting tensorboard<2.1.0,>=2.0.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a7/3d/993131c622ae34f9401a81526853f2310a4834bd042420a7982fb5bc5fd0/tensorboard-2.0.2-py2-none-any.whl (3.8MB)\n","\u001b[K     |████████████████████████████████| 3.8MB 42.6MB/s \n","\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python2.7/dist-packages (from tensorflow<2.1.0,>=2.0.0->turicreate) (1.11.2)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python2.7/dist-packages (from tensorflow<2.1.0,>=2.0.0->turicreate) (3.8.0)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python2.7/dist-packages (from tensorflow<2.1.0,>=2.0.0->turicreate) (1.1.0)\n","Requirement already satisfied: backports.weakref>=1.0rc1; python_version < \"3.4\" in /usr/local/lib/python2.7/dist-packages (from tensorflow<2.1.0,>=2.0.0->turicreate) (1.0.post1)\n","Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python2.7/dist-packages (from tensorflow<2.1.0,>=2.0.0->turicreate) (1.0.8)\n","Requirement already satisfied: wheel in /usr/local/lib/python2.7/dist-packages (from tensorflow<2.1.0,>=2.0.0->turicreate) (0.35.1)\n","Requirement already satisfied: functools32>=3.2.3 in /usr/local/lib/python2.7/dist-packages (from tensorflow<2.1.0,>=2.0.0->turicreate) (3.2.3.post2)\n","Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow<2.1.0,>=2.0.0->turicreate) (2.0.0)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow<2.1.0,>=2.0.0->turicreate) (0.7.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python2.7/dist-packages (from tensorflow<2.1.0,>=2.0.0->turicreate) (2.3.2)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow<2.1.0,>=2.0.0->turicreate) (1.1.0)\n","Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow<2.1.0,>=2.0.0->turicreate) (0.1.7)\n","Requirement already satisfied: enum34>=1.1.6; python_version < \"3.4\" in /usr/local/lib/python2.7/dist-packages (from tensorflow<2.1.0,>=2.0.0->turicreate) (1.1.6)\n","Collecting tensorflow-estimator<2.1.0,>=2.0.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/08/8b927337b7019c374719145d1dceba21a8bb909b93b1ad6f8fb7d22c1ca1/tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449kB)\n","\u001b[K     |████████████████████████████████| 450kB 34.3MB/s \n","\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow<2.1.0,>=2.0.0->turicreate) (0.8.1)\n","Requirement already satisfied: pytz>=2011k in /usr/local/lib/python2.7/dist-packages (from pandas>=0.23.2->turicreate) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python2.7/dist-packages (from pandas>=0.23.2->turicreate) (2.5.3)\n","Requirement already satisfied: funcsigs in /usr/local/lib/python2.7/dist-packages (from numba<0.51.0->turicreate) (1.0.2)\n","Requirement already satisfied: singledispatch in /usr/local/lib/python2.7/dist-packages (from numba<0.51.0->turicreate) (3.4.0.3)\n","Requirement already satisfied: futures>=2.2.0 in /usr/local/lib/python2.7/dist-packages (from grpcio>=1.8.6->tensorflow<2.1.0,>=2.0.0->turicreate) (3.2.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python2.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->turicreate) (44.1.1)\n","Collecting google-auth-oauthlib<0.5,>=0.4.1\n","  Downloading https://files.pythonhosted.org/packages/7b/b8/88def36e74bee9fce511c9519571f4e485e890093ab7442284f4ffaef60b/google_auth_oauthlib-0.4.1-py2.py3-none-any.whl\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python2.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->turicreate) (0.15.5)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python2.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->turicreate) (1.17.2)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python2.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->turicreate) (3.1.1)\n","Requirement already satisfied: h5py in /usr/local/lib/python2.7/dist-packages (from keras-applications>=1.0.8->tensorflow<2.1.0,>=2.0.0->turicreate) (2.8.0)\n","Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow<2.1.0,>=2.0.0->turicreate) (5.4.0)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python2.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->turicreate) (1.2.0)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->turicreate) (3.1.1)\n","Requirement already satisfied: rsa<4.1; python_version < \"3\" in /usr/local/lib/python2.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->turicreate) (4.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python2.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->turicreate) (0.2.5)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python2.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->turicreate) (3.0.2)\n","Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python2.7/dist-packages (from rsa<4.1; python_version < \"3\"->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->turicreate) (0.4.5)\n","\u001b[31mERROR: fastai 0.7.0 has requirement torch<0.4, but you'll have torch 1.4.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: tensorboard 2.0.2 has requirement grpcio>=1.24.3, but you'll have grpcio 1.15.0 which is incompatible.\u001b[0m\n","Installing collected packages: pillow, google-auth-oauthlib, tensorboard, tensorflow-estimator, tensorflow, llvmlite, coremltools, turicreate\n","  Found existing installation: Pillow 4.3.0\n","    Uninstalling Pillow-4.3.0:\n","      Successfully uninstalled Pillow-4.3.0\n","  Found existing installation: google-auth-oauthlib 0.4.0\n","    Uninstalling google-auth-oauthlib-0.4.0:\n","      Successfully uninstalled google-auth-oauthlib-0.4.0\n","  Found existing installation: tensorboard 2.1.0\n","    Uninstalling tensorboard-2.1.0:\n","      Successfully uninstalled tensorboard-2.1.0\n","  Found existing installation: tensorflow-estimator 1.15.0\n","    Uninstalling tensorflow-estimator-1.15.0:\n","      Successfully uninstalled tensorflow-estimator-1.15.0\n","  Found existing installation: tensorflow 2.1.0\n","    Uninstalling tensorflow-2.1.0:\n","      Successfully uninstalled tensorflow-2.1.0\n","  Found existing installation: llvmlite 0.29.0\n","    Uninstalling llvmlite-0.29.0:\n","      Successfully uninstalled llvmlite-0.29.0\n","Successfully installed coremltools-3.3 google-auth-oauthlib-0.4.1 llvmlite-0.31.0 pillow-6.2.2 tensorboard-2.0.2 tensorflow-2.0.0 tensorflow-estimator-2.0.1 turicreate-6.4.1\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["PIL"]}}},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"QY0EW61HVkUY"},"source":["# Load in house sales data\n","\n","Dataset is from house sales in King County, the region where the city of Seattle, WA is located."]},{"cell_type":"code","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/","height":343},"id":"eV5spqi4VkUY","executionInfo":{"status":"error","timestamp":1606542083518,"user_tz":300,"elapsed":1095,"user":{"displayName":"Gloria Lee","photoUrl":"","userId":"14673960919881103227"}},"outputId":"d575e2de-773b-447c-87d6-5c3e6f97f15b"},"source":["\n","sales = turicreate.SFrame('home_data.sframe/')"],"execution_count":9,"outputs":[{"output_type":"error","ename":"IOError","evalue":"ignored","traceback":["\u001b[0;31m\u001b[0m","\u001b[0;31mIOError\u001b[0mTraceback (most recent call last)","\u001b[0;32m<ipython-input-9-f9df5a3509cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msales\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mturicreate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'home_data.sframe'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python2.7/dist-packages/turicreate/data_structures/sframe.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, format, _proxy)\u001b[0m\n\u001b[1;32m    836\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 838\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unknown input type: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python2.7/dist-packages/turicreate/_cython/context.pyc\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_cython_trace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0;31m# To hide cython trace, we re-raise from here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0;31m# To show the full trace, we do nothing and let exception propagate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIOError\u001b[0m: /content/home_data.sframe not found. ErrMsg: file not exist in /content/home_data.sframe"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":453},"id":"-HHkgoiXWYTo","executionInfo":{"status":"error","timestamp":1606542145800,"user_tz":300,"elapsed":614,"user":{"displayName":"Gloria Lee","photoUrl":"","userId":"14673960919881103227"}},"outputId":"bc460f20-6227-44ac-c899-0c86e45ef80f"},"source":["from pathlib import Path\n","import os\n","folder_path = os.path.abspath('REG05-NB01')\n","file_path = os.path.dirname(os.path.dirname(folder_path))\n","\n","print(folder_path)\n","\n","import pandas as pd\n","sales = pd.read_csv('kc_house_data.csv')"],"execution_count":10,"outputs":[{"output_type":"stream","text":["/content/REG05-NB01\n"],"name":"stdout"},{"output_type":"error","ename":"IOError","evalue":"ignored","traceback":["\u001b[0;31m\u001b[0m","\u001b[0;31mIOError\u001b[0mTraceback (most recent call last)","\u001b[0;32m<ipython-input-10-7b056a030a19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0msales\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'kc_house_data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n","\u001b[0;31mIOError\u001b[0m: [Errno 2] File kc_house_data.csv does not exist: 'kc_house_data.csv'"]}]},{"cell_type":"markdown","metadata":{"id":"tjy3gugIVkUY"},"source":["# Create new features"]},{"cell_type":"markdown","metadata":{"id":"BwApdhMnVkUY"},"source":["As in Week 2, we consider features that are some transformations of inputs."]},{"cell_type":"code","metadata":{"id":"yYJPumatVkUY"},"source":["from math import log, sqrt\n","sales['sqft_living_sqrt'] = sales['sqft_living'].apply(sqrt)\n","sales['sqft_lot_sqrt'] = sales['sqft_lot'].apply(sqrt)\n","sales['bedrooms_square'] = sales['bedrooms']*sales['bedrooms']\n","\n","# In the dataset, 'floors' was defined with type string, \n","# so we'll convert them to float, before creating a new feature.\n","sales['floors'] = sales['floors'].astype(float) \n","sales['floors_square'] = sales['floors']*sales['floors']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wI19Xe2eVkUZ"},"source":["* Squaring bedrooms will increase the separation between not many bedrooms (e.g. 1) and lots of bedrooms (e.g. 4) since 1^2 = 1 but 4^2 = 16. Consequently this variable will mostly affect houses with many bedrooms.\n","* On the other hand, taking square root of sqft_living will decrease the separation between big house and small house. The owner may not be exactly twice as happy for getting a house that is twice as big."]},{"cell_type":"markdown","metadata":{"id":"GY-wPSzlVkUZ"},"source":["# Learn regression weights with L1 penalty"]},{"cell_type":"markdown","metadata":{"id":"v4LAJaRnVkUZ"},"source":["Let us fit a model with all the features available, plus the features we just created above."]},{"cell_type":"code","metadata":{"id":"at7HUzpZVkUZ"},"source":["all_features = ['bedrooms', 'bedrooms_square',\n","                'bathrooms',\n","                'sqft_living', 'sqft_living_sqrt',\n","                'sqft_lot', 'sqft_lot_sqrt',\n","                'floors', 'floors_square',\n","                'waterfront', 'view', 'condition', 'grade',\n","                'sqft_above',\n","                'sqft_basement',\n","                'yr_built', 'yr_renovated']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2JP2vYAkVkUZ"},"source":["Applying L1 penalty requires adding an extra parameter (`l1_penalty`) to the linear regression call in Turi Create. (Other tools may have separate implementations of LASSO.)  Note that it's important to set `l2_penalty=0` to ensure we don't introduce an additional L2 penalty."]},{"cell_type":"code","metadata":{"id":"0FFHt3uWVkUZ"},"source":["model_all = turicreate.linear_regression.create(sales, target='price', features=all_features,\n","                                                validation_set=None, \n","                                                l2_penalty=0., l1_penalty=1e10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F5Ko0YXHVkUZ"},"source":["Find what features had non-zero weight."]},{"cell_type":"code","metadata":{"id":"_aMpzA8EVkUZ"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"afPV8cUYVkUZ"},"source":["Note that a majority of the weights have been set to zero. So by setting an L1 penalty that's large enough, we are performing a subset selection. \n","\n","***QUIZ QUESTION***:\n","According to this list of weights, which of the features have been chosen? "]},{"cell_type":"markdown","metadata":{"id":"1A71rwFYVkUZ"},"source":["# Selecting an L1 penalty"]},{"cell_type":"markdown","metadata":{"id":"npot5ityVkUZ"},"source":["To find a good L1 penalty, we will explore multiple values using a validation set. Let us do three way split into train, validation, and test sets:\n","* Split our sales data into 2 sets: training and test\n","* Further split our training data into two sets: train, validation\n","\n","Be *very* careful that you use seed = 1 to ensure you get the same answer!"]},{"cell_type":"code","metadata":{"id":"je00CIdXVkUZ"},"source":["(training_and_validation, testing) = sales.random_split(.9,seed=1) # initial train/test split\n","(training, validation) = training_and_validation.random_split(0.5, seed=1) # split training into train and validate"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qqRyWpXoVkUZ"},"source":["Next, we write a loop that does the following:\n","* For `l1_penalty` in [10^1, 10^1.5, 10^2, 10^2.5, ..., 10^7] (to get this in Python, type `np.logspace(1, 7, num=13)`.)\n","    * Fit a regression model with a given `l1_penalty` on TRAIN data. Specify `l1_penalty=l1_penalty` and `l2_penalty=0.` in the parameter list.\n","    * Compute the RSS on VALIDATION data (here you will want to use `.predict()`) for that `l1_penalty`\n","* Report which `l1_penalty` produced the lowest RSS on validation data.\n","\n","When you call `linear_regression.create()` make sure you set `validation_set = None`.\n","\n","Note: you can turn off the print out of `linear_regression.create()` with `verbose = False`"]},{"cell_type":"code","metadata":{"id":"sVNQKTDyVkUZ"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nfraMrCSVkUZ"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rs_570K5VkUZ"},"source":["*** QUIZ QUESTION. *** What was the best value for the `l1_penalty`?"]},{"cell_type":"code","metadata":{"id":"ASUlO8qSVkUZ"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l58vx6MdVkUZ"},"source":["***QUIZ QUESTION***\n","Also, using this value of L1 penalty, how many nonzero weights do you have?"]},{"cell_type":"code","metadata":{"id":"_0RKdhfmVkUZ"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RWuYM2hYVkUa"},"source":["# Limit the number of nonzero weights\n","\n","What if we absolutely wanted to limit ourselves to, say, 7 features? This may be important if we want to derive \"a rule of thumb\" --- an interpretable model that has only a few features in them."]},{"cell_type":"markdown","metadata":{"id":"8ogkGQzjVkUa"},"source":["In this section, you are going to implement a simple, two phase procedure to achive this goal:\n","1. Explore a large range of `l1_penalty` values to find a narrow region of `l1_penalty` values where models are likely to have the desired number of non-zero weights.\n","2. Further explore the narrow region you found to find a good value for `l1_penalty` that achieves the desired sparsity.  Here, we will again use a validation set to choose the best value for `l1_penalty`."]},{"cell_type":"code","metadata":{"id":"G9sZcLZVVkUa"},"source":["max_nonzeros = 7"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RB4MyL0aVkUa"},"source":["## Exploring the larger range of values to find a narrow range with the desired sparsity\n","\n","Let's define a wide range of possible `l1_penalty_values`:"]},{"cell_type":"code","metadata":{"id":"00meBWuDVkUa"},"source":["l1_penalty_values = np.logspace(8, 10, num=20)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zzTKSbV9VkUa"},"source":["Now, implement a loop that search through this space of possible `l1_penalty` values:\n","\n","* For `l1_penalty` in `np.logspace(8, 10, num=20)`:\n","    * Fit a regression model with a given `l1_penalty` on TRAIN data. Specify `l1_penalty=l1_penalty` and `l2_penalty=0.` in the parameter list. When you call `linear_regression.create()` make sure you set `validation_set = None`\n","    * Extract the weights of the model and count the number of nonzeros. Save the number of nonzeros to a list.\n","        * *Hint: `model.coefficients['value']` gives you an SArray with the parameters you learned.  If you call the method `.nnz()` on it, you will find the number of non-zero parameters!* "]},{"cell_type":"code","metadata":{"id":"s0DzthwbVkUa"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kiEcoEsxVkUa"},"source":["Out of this large range, we want to find the two ends of our desired narrow range of `l1_penalty`.  At one end, we will have `l1_penalty` values that have too few non-zeros, and at the other end, we will have an `l1_penalty` that has too many non-zeros.  \n","\n","More formally, find:\n","* The largest `l1_penalty` that has more non-zeros than `max_nonzeros` (if we pick a penalty smaller than this value, we will definitely have too many non-zero weights)\n","    * Store this value in the variable `l1_penalty_min` (we will use it later)\n","* The smallest `l1_penalty` that has fewer non-zeros than `max_nonzeros` (if we pick a penalty larger than this value, we will definitely have too few non-zero weights)\n","    * Store this value in the variable `l1_penalty_max` (we will use it later)\n","\n","\n","*Hint: there are many ways to do this, e.g.:*\n","* Programmatically within the loop above\n","* Creating a list with the number of non-zeros for each value of `l1_penalty` and inspecting it to find the appropriate boundaries."]},{"cell_type":"code","metadata":{"id":"cKLczlFvVkUa"},"source":["l1_penalty_min = \n","l1_penalty_max = "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FyepMW_UVkUa"},"source":["***QUIZ QUESTION.*** What values did you find for `l1_penalty_min` and `l1_penalty_max`, respectively? "]},{"cell_type":"markdown","metadata":{"id":"Q1Ll-uB9VkUa"},"source":["## Exploring the narrow range of values to find the solution with the right number of non-zeros that has lowest RSS on the validation set \n","\n","We will now explore the narrow region of `l1_penalty` values we found:"]},{"cell_type":"code","metadata":{"id":"HKDRR8pdVkUa"},"source":["l1_penalty_values = np.linspace(l1_penalty_min,l1_penalty_max,20)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kLAiY-1zVkUa"},"source":["* For `l1_penalty` in `np.linspace(l1_penalty_min,l1_penalty_max,20)`:\n","    * Fit a regression model with a given `l1_penalty` on TRAIN data. Specify `l1_penalty=l1_penalty` and `l2_penalty=0.` in the parameter list. When you call `linear_regression.create()` make sure you set `validation_set = None`\n","    * Measure the RSS of the learned model on the VALIDATION set\n","\n","Find the model that the lowest RSS on the VALIDATION set and has sparsity *equal* to `max_nonzeros`."]},{"cell_type":"code","metadata":{"id":"hC49A1vWVkUa"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WZynDIpXVkUa"},"source":["***QUIZ QUESTIONS***\n","1. What value of `l1_penalty` in our narrow range has the lowest RSS on the VALIDATION set and has sparsity *equal* to `max_nonzeros`?\n","2. What features in this model have non-zero coefficients?"]},{"cell_type":"code","metadata":{"id":"GGdgn_tZVkUa"},"source":[""],"execution_count":null,"outputs":[]}]}